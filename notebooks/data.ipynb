{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/yahoo_answers.csv\")\n",
    "\n",
    "human_answers = df[\"answer\"].tolist()\n",
    "for i, answer in enumerate(human_answers):\n",
    "  with open(\"../data/responses.csv\", \"a\") as f:\n",
    "    f.write(f'{i},\"{answer}\",human\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/yahoo_answers.csv\")\n",
    "\n",
    "df[\"question\"] = df[\"header\"] + \" \" + df[\"subheader\"] if type(df[\"subheader\"]) == \"string\" else \"\"\n",
    "questions = df[\"question\"].tolist()\n",
    "\n",
    "DELIMETER = \"|\"\n",
    "for i, answer in enumerate(df[\"answer\"]):\n",
    "  if type(answer) == str:\n",
    "    answer = answer.replace(\"|\", \"\")\n",
    "  with open(\"../data/responses.csv\", \"a\") as f:\n",
    "    f.write(f'{i}{DELIMETER}\"{answer}\"{DELIMETER}human\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "df = pd.read_csv(\"../data/yahoo_answers.csv\")\n",
    "df[\"question\"] = df[\"header\"] + (\" \" + df[\"subheader\"] if type(df[\"subheader\"]) == str else \"\")\n",
    "questions = df[\"question\"]\n",
    "\n",
    "DELIMETER = \"|\"\n",
    "\n",
    "responses_df = pd.read_csv(\"../data/responses.csv\", sep=DELIMETER)\n",
    "num_answers = len(responses_df)\n",
    "responses_df = responses_df[responses_df[\"category\"] == \"ChatGPT\"]\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "  if i % 100 == 0:\n",
    "    print(f\"Processing question {i} of {len(questions)}\")\n",
    "  if i < len(responses_df):\n",
    "    continue\n",
    "  should_break = False\n",
    "  while not should_break:\n",
    "    try:\n",
    "      response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "              {\"role\": \"system\", \"content\": \"Respond to the following question like you are a human answering a short essay prompt.\"},\n",
    "              {\"role\": \"user\", \"content\": question},\n",
    "          ]\n",
    "      )\n",
    "      should_break = True\n",
    "    except Exception as e:\n",
    "      # if the OpenAI API is overloaded, wait 2 minutes and try again\n",
    "      print(e)\n",
    "      sleep(120)\n",
    "\n",
    "  answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "  items_to_remove = [\n",
    "    \"As an AI language model, \",\n",
    "    \"Sorry, as an AI language model \",\n",
    "    \"I'm sorry, as an AI language model, \",\n",
    "    \"I'm sorry, but as an AI language model, \",\n",
    "    \"I do not have personal preferences or emotions. \",\n",
    "    \"I do not have personal preferences or emotions, but \",\n",
    "    \"I do not have personal beliefs or opinions. \",\n",
    "    \"As a language model AI, \",\n",
    "    \"|\",\n",
    "    \"\\n\"\n",
    "  ]\n",
    "  for item in items_to_remove:\n",
    "    answer = answer.replace(item, \"\")\n",
    "  answer = answer.replace('\"', \"'\")\n",
    "  answer = answer.replace(\":\", \": \")\n",
    "\n",
    "  with open(\"../data/responses.csv\", \"a\") as f:\n",
    "    f.write(f'{num_answers+i}{DELIMETER}\"{answer}\"{DELIMETER}ChatGPT\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/responses.csv\", sep=\"|\")\n",
    "ai = df[df[\"category\"] == \"ChatGPT\"]\n",
    "human = df[df[\"category\"] == \"human\"]\n",
    "number_to_use = math.floor((len(ai) / len(human)) * len(human))\n",
    "\n",
    "human_to_use = random.sample(human[\"text\"].tolist(), number_to_use)\n",
    "ai_to_use = ai[\"text\"].tolist()\n",
    "\n",
    "# # create a dataframe with human_to_use with the category \"human\" and ai_to_use with the category \"ChatGPT\"\n",
    "data = []\n",
    "data += [[item, \"human\"] for item in human_to_use]\n",
    "data += [[item, \"ChatGPT\"] for item in ai_to_use]\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"category\"])\n",
    "\n",
    "df.to_csv(\"../data/data.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv(\"../data/data.csv\", sep=\"|\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(str(text))\n",
    "    return \",\".join([token.text for token in doc])\n",
    "\n",
    "df[\"tokens\"] = df[\"text\"].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the median number of tokens in each category\n",
    "human_median = df[df[\"category\"] == \"human\"][\"tokens\"].apply(lambda x: len(x)).median()\n",
    "ai_median = df[df[\"category\"] == \"ChatGPT\"][\"tokens\"].apply(lambda x: len(x)).median()\n",
    "\n",
    "# graph the median length of each category\n",
    "plt.bar([\"human\", \"ChatGPT\"], [human_median, ai_median])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-HbUNm4sg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
